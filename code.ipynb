{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0Jz95-wVjtb"
      },
      "source": [
        "# Deep Learning Clothes Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiUQO0pJ-Evq"
      },
      "source": [
        "This notebook requires Pytorch, torchtext, and timm libraries. Training the models requires at least 6 GB of GPU memory. I trained the models on my laptop, but the code should also run on Google Colab with a GPU setting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86tJixQ6CiHk"
      },
      "source": [
        "GPU on my laptop: NVIDIA GeForce RTX 3060"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-3fodrJBJaZ"
      },
      "source": [
        "Library versions on my laptop:\n",
        "- pytorch==1.13.1\n",
        "- torchtext==0.6.0\n",
        "- timm==0.6.12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFC_LMCw_2n6"
      },
      "source": [
        "The code expects the following directory structure:\n",
        "- code.ipynb\n",
        "- models\n",
        "- dataset\n",
        "    - noisy-images\n",
        "        - noisy-images\n",
        "            - 1163.jpg\n",
        "            - ...\n",
        "    - test.csv\n",
        "    - train.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT0JAPoiVjtf"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi1PaQa-Vvk3",
        "outputId": "332be76a-ba46-4888-ca76-6d1fa34860a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# google calab only, don't run this locally\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmEqklvtWi0l",
        "outputId": "44f861c4-4087-4a03-e323-abb0486b3786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.6.13-py3-none-any.whl (549 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from timm) (6.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from timm) (0.15.1+cu118)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.9/dist-packages (from timm) (2.0.0+cu118)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (3.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.13.4 timm-0.6.13\n"
          ]
        }
      ],
      "source": [
        "# google calab only, don't run this locally\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "657z8M3nVjtg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "from PIL import Image\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import transforms\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.vocab import Vocab\n",
        "\n",
        "# pytorch image models\n",
        "import timm\n",
        "\n",
        "# ensemble\n",
        "from scipy.stats import mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Mcu45AGEVjti"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THpM-mT3Vjti"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gllmDi1RVjti"
      },
      "source": [
        "Preprocess the noisy text description in the train and test dataset. Remove prunctuations and rare words. Appends the categorical data to the front of the noisy text description. Write the cleaned description to new dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qxRtEAJdVjtj"
      },
      "outputs": [],
      "source": [
        "# train data\n",
        "train_data = pd.read_csv('./dataset/train.csv')\n",
        "\n",
        "# words in categorical data\n",
        "common_words = {c.lower() for c in train_data['baseColour']}\n",
        "common_words.update({use.lower() for use in train_data['usage']})\n",
        "common_words.update({s.lower() for s in train_data['season']})\n",
        "common_words = list(common_words)\n",
        "common_words.extend(['women', 'woman', 'womens', 'men', 'man', 'mens'\n",
        "                     'unisex', 'girls', 'girl', 'boys', 'boy'])\n",
        "\n",
        "# rare words in description\n",
        "occurrenecs = dict()\n",
        "for i in range(len(train_data)):\n",
        "    description = train_data.iloc[i]['noisyTextDescription'].lower()\n",
        "    tokens = description.lower().split()\n",
        "    for token in tokens:\n",
        "        if token not in occurrenecs:\n",
        "            occurrenecs[token] = 1\n",
        "        else:\n",
        "            occurrenecs[token] += 1\n",
        "rare_words = [key for key, value in occurrenecs.items() if value <= 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_0yxZ5z1Vjtj"
      },
      "outputs": [],
      "source": [
        "def clean_tokens(text: str, common_words: list, rare_words: list) -> list:\n",
        "    \"\"\"\n",
        "    Removes punctuations, rare words and common words from a single text description.\n",
        "    Splits the text description into a list of tokens.\n",
        "    \n",
        "    :param text: A single noisy text description\n",
        "    :param common_words: A list of words that occur in the categorical data,\n",
        "        will remove them from the noisy text description\n",
        "    :param rare_words: A list of rare words in the noisy text description,\n",
        "        will remove them from the noisy text description\n",
        "    :return: A list of tokens corresponding to a single text description\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    # remove rare words and any words that contain r\"[0-9\\/\\=\\*\\\"\\n]\"\n",
        "    new_tokens = []\n",
        "    for token in text.split():\n",
        "        if (token not in rare_words and\n",
        "            not re.search(r\"[0-9\\/\\=\\*\\\"\\n]\", token)):\n",
        "            new_tokens.append(token)\n",
        "    text = \" \".join(new_tokens)\n",
        "    # remove \"'s\"\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    # remove \"\\&\"\n",
        "    text = re.sub(r\"\\&+\", \" \", text)\n",
        "    # remove \"\\-\"\n",
        "    text = re.sub(r\"\\-+\", \" \", text)\n",
        "    # remove \"\\+\"\n",
        "    text = re.sub(r\"\\++\", \" \", text)\n",
        "    # remove words that appear in categorical data\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in common_words:\n",
        "            tokens.append(token)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DTj-okbsVjtk"
      },
      "outputs": [],
      "source": [
        "def clean_data(dataframe: pd.DataFrame, common_words: list, rare_words: list) -> tuple:\n",
        "    \"\"\"\n",
        "    Cleans the noisy text description column in a dataframe and\n",
        "    puts the cleaned desciption in a new dataframe.\n",
        "    Removes punctuations, rare words and common words from the noisy text description.\n",
        "    Appends categorical data to the front of text description.\n",
        "\n",
        "    :param dataframe: A single dataframe that contains the columns\n",
        "        'gender', 'season', 'usage', 'baseColour', and 'noisyTextDescription'\n",
        "    :param common_words: A list of words that occur in the categorical data,\n",
        "        will remove them from the noisy text description\n",
        "    :param rare_words: A list of rare words in the noisy text description,\n",
        "        will remove them from the noisy text description\n",
        "    :return: Tuple of the following:\n",
        "    * A new dataframe with a column 'description' added to the original dataframe\n",
        "    * Maximum number of tokens in a single entry under the column 'description'\n",
        "    \"\"\"\n",
        "    description = []\n",
        "    max_seq = 0\n",
        "    for i in range(len(dataframe)):\n",
        "        gender = dataframe.iloc[i]['gender'].lower()\n",
        "        season = dataframe.iloc[i]['season'].lower()\n",
        "        usage = dataframe.iloc[i]['usage'].lower()\n",
        "        colour = dataframe.iloc[i]['baseColour'].lower()\n",
        "        text = dataframe.iloc[i]['noisyTextDescription'].lower()\n",
        "        new_tokens = clean_tokens(text, common_words, rare_words)\n",
        "        # append words in categorical data\n",
        "        tokens = [gender, season, usage, colour]\n",
        "        tokens.extend(new_tokens)\n",
        "        max_seq = max(max_seq, len(tokens))\n",
        "        text = \" \".join(tokens)\n",
        "        description.append(text)\n",
        "    dataframe['description'] = description\n",
        "    return dataframe, max_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozG0iZMsVjtk",
        "outputId": "9df9ec94-fa62-45cc-9d04-ba7a8ee885a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data max sequence: 15\n",
            "Test data max sequence: 15\n"
          ]
        }
      ],
      "source": [
        "# clean train data\n",
        "train_data, max_seq = clean_data(train_data, common_words, rare_words)\n",
        "print('Train data max sequence:', max_seq)\n",
        "\n",
        "# clean test data\n",
        "test_data = pd.read_csv('./dataset/test.csv')\n",
        "test_data, max_seq = clean_data(test_data, common_words, rare_words)\n",
        "print('Test data max sequence:', max_seq)\n",
        "\n",
        "# save\n",
        "train_ratio = 0.8\n",
        "train_len = int(len(train_data) * train_ratio)\n",
        "train_data.iloc[:train_len].to_csv('./dataset/train_cleaned.csv', index=False)\n",
        "train_data.iloc[train_len:].to_csv('./dataset/val_cleaned.csv', index=False)\n",
        "test_data.to_csv('./dataset/test_cleaned.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwnsnNqkVjtl"
      },
      "source": [
        "## Build vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RRur3SwhVjtl"
      },
      "outputs": [],
      "source": [
        "def yield_description_tokens(dataframe: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Generates lists of tokens for each description entry in a dataframe.\n",
        "\n",
        "    :param dataframe: Training data that contains column 'description'\n",
        "    \"\"\"\n",
        "    for i in range(len(dataframe)):\n",
        "        description = dataframe.iloc[i]['description']\n",
        "        yield description.split()\n",
        "\n",
        "def yield_category_tokens(dataframe: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Generates category for each target entry in a dataframe.\n",
        "\n",
        "    :param dataframe: Training data that contains column 'target'\n",
        "    \"\"\"\n",
        "    for i in range(len(dataframe)):\n",
        "        target = dataframe.iloc[i]['category']\n",
        "        yield [target]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQOcOJcQVjtm"
      },
      "source": [
        "From training data, build vocabulary, i.e. mapping from tokens to indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rwXzeyh3Vjtm"
      },
      "outputs": [],
      "source": [
        "### run this locally, for older version of torchtext\n",
        "# builds vocabulary for text description from training data\n",
        "description_vocab = build_vocab_from_iterator(yield_description_tokens(train_data))\n",
        "# builds vocubulary\n",
        "category_vocab = build_vocab_from_iterator(yield_category_tokens(train_data))\n",
        "category_vocab = Vocab(category_vocab.freqs, specials=[])\n",
        "\n",
        "### run this on Colab, for new version of torchtext\n",
        "# # builds vocabulary for text description from training data\n",
        "# description_vocab = build_vocab_from_iterator(yield_description_tokens(train_data), specials=['<unk>', '<pad>'])\n",
        "# description_vocab.set_default_index(0)\n",
        "# # builds vocubulary\n",
        "# category_vocab = build_vocab_from_iterator(yield_category_tokens(train_data), specials=[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDgForvSVjtm",
        "outputId": "ae897b1b-a078-4b6e-de8c-3b61fd0433d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The first few tokens in the text description vocabulary:\n",
            "['<unk>', '<pad>', 'casual', 'men', 'summer', 'women', 'fall', 'black', 'winter', 'blue']\n",
            "The number of tokens in the text description vocabulary: 5678\n",
            "\n",
            "All tokens in the target category vocabulary:\n",
            "['Topwear', 'Shoes', 'Bags', 'Bottomwear', 'Watches', 'Innerwear', 'Eyewear', 'Fragrance', 'Jewellery', 'Sandal', 'Flip Flops', 'Wallets', 'Belts', 'Socks', 'Loungewear and Nightwear', 'Dress', 'Lips', 'Saree', 'Makeup', 'Nails', 'Ties', 'Headwear', 'Accessories', 'Scarves', 'Cufflinks', 'Apparel Set', 'Free Gifts']\n",
            "The number of tokens in the target category vocabulary: 27\n"
          ]
        }
      ],
      "source": [
        "### run this locally, for older version of torchtext\n",
        "# note that text description vocabulary includes unknown token <unk> and pad token <pad>\n",
        "# target categories do not include <unk> or <pad>\n",
        "print('The first few tokens in the text description vocabulary:')\n",
        "print(description_vocab.itos[:10])\n",
        "print('The number of tokens in the text description vocabulary:', len(description_vocab.itos))\n",
        "print('\\nAll tokens in the target category vocabulary:')\n",
        "print(category_vocab.itos)\n",
        "print('The number of tokens in the target category vocabulary:', len(category_vocab.itos))\n",
        "\n",
        "### run this on Colab, for new version of torchtext\n",
        "# # note that text description vocabulary includes unknown token <unk> and pad token <pad>\n",
        "# # target categories do not include <unk> or <pad>\n",
        "# print('The first few tokens in the text description vocabulary:')\n",
        "# print(description_vocab.get_itos()[:10])\n",
        "# print('The number of tokens in the text description vocabulary:', len(description_vocab.get_itos()))\n",
        "# print('\\nAll tokens in the target category vocabulary:')\n",
        "# print(category_vocab.get_itos())\n",
        "# print('The number of tokens in the target category vocabulary:', len(category_vocab.get_itos()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hx6BYiGVjtm"
      },
      "source": [
        "## Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6ycbIbJiVjtn"
      },
      "outputs": [],
      "source": [
        "# custom dataset, loads images, text descriptions, and categories\n",
        "# apply resize, random horizontal flip and random noise to images\n",
        "\n",
        "class ImageTextDataset(Dataset):\n",
        "    def __init__(self, dataframe_path: str, image_root: str,\n",
        "                 text_vocab: Vocab, category_vocab: Vocab,\n",
        "                 max_seq: int, noise_prob:float, transform, train=True):\n",
        "        self.dataframe = pd.read_csv(dataframe_path)\n",
        "        self.text_vocab = text_vocab\n",
        "        self.texts = torch.ones(len(self.dataframe), max_seq, dtype=torch.long)\n",
        "        self.image_root = image_root\n",
        "        self.transform_image = transform\n",
        "        self.category_vocab = category_vocab\n",
        "        self.max_seq = max_seq\n",
        "        self.noise = None\n",
        "        self.noise_prob = noise_prob\n",
        "        self.train = train\n",
        "        for i in range(len(self.dataframe)):\n",
        "            # text description\n",
        "            description = self.dataframe.iloc[i]['description']\n",
        "            tokens = description.split()\n",
        "            for j, token in enumerate(tokens):\n",
        "                self.texts[i][j] = text_vocab[token]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # image\n",
        "        image_idx = int(self.dataframe.iloc[idx]['id'])\n",
        "        image_path = f'{self.image_root}/{image_idx}.jpg'\n",
        "        image = Image.open(image_path)\n",
        "        image = self.transform_image(image)\n",
        "        self.noise = image\n",
        "        prob = np.random.uniform(0, 1, 1)\n",
        "        if prob < self.noise_prob and self.noise is not None:\n",
        "            image_transformed = 0.95 * image + 0.05 * self.noise\n",
        "        else:\n",
        "            image_transformed = image\n",
        "        # text description\n",
        "        text = self.texts[idx]\n",
        "        if not self.train:\n",
        "            return image_transformed, text\n",
        "        # category\n",
        "        category = self.dataframe.iloc[idx]['category']\n",
        "        category = self.category_vocab[category]\n",
        "\n",
        "        return image_transformed, text, category"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0t0J3CmVjtn"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LaUqz-YVjtn"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrxkFv-G9ff7"
      },
      "source": [
        "Transformer model for text data, only uses encoding layers but does not use decoding layers. Added a fully connected classification layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "X4-g5q_LVjtn"
      },
      "outputs": [],
      "source": [
        "# transformer positional embedding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, max_length: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_length, d_model)\n",
        "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1hmtGO68Vjtn"
      },
      "outputs": [],
      "source": [
        "# Transformer text classifier\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        d_model: int,\n",
        "        output_size: int,\n",
        "        max_length: int,\n",
        "        nhead: int = 8,\n",
        "        dim_feedforward: int = 512,\n",
        "        num_layers: int = 6,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert d_model % nhead == 0, \"nheads must divide evenly into d_model\"\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(\n",
        "            d_model=d_model, max_length=max_length, dropout=dropout\n",
        "        )\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, output_size)\n",
        "        self.d_model = d_model\n",
        "        self.num_features = d_model\n",
        "        self.num_classes = output_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "    \n",
        "    def forward_features(self, x):\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWnsycuWVjto"
      },
      "source": [
        "### Image Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_lmn8RlVjto"
      },
      "source": [
        "A wrapper class for the image models (Resnet, Efficientnet, etc.) in timm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "u0Rxz6_UzdCk"
      },
      "outputs": [],
      "source": [
        "class ImageModel(nn.Module):\n",
        "    def __init__(self, backbone, output_size=27):\n",
        "        super(ImageModel, self).__init__()\n",
        "        self.backbone = backbone\n",
        "        self.fc1 = nn.Linear(1000, 256)\n",
        "        self.fc2 = nn.Linear(256, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.backbone(x)\n",
        "        output = self.relu(self.fc1(output))\n",
        "        return self.fc2(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQRGX53nVjto"
      },
      "source": [
        "### Image Text Models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zkMYrZN6Vjto"
      },
      "source": [
        "Combine image models with text models. Concatenate the embeddings of image and text models. Add a classifier to the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qnl7Xa8MVjto"
      },
      "outputs": [],
      "source": [
        "# image text model\n",
        "class ImageTextClassifier(nn.Module):\n",
        "    def __init__(self, image_model, text_model, hidden_size: int,\n",
        "                 num_classes: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.image_model = image_model\n",
        "        self.text_model = text_model\n",
        "        self.num_features = image_model.num_features + text_model.num_features\n",
        "        self.fc1 = nn.Linear(self.num_features, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_size, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, image, text):\n",
        "        x1 = self.image_model.forward_features(image)\n",
        "        x1 = self.image_model.forward_head(x1, pre_logits=True).flatten(start_dim=1)\n",
        "        x2 = self.text_model.forward_features(text)\n",
        "        x = torch.concat((x1, x2), dim=1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHOoy6PfVjto"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JxRUqwbWVjto"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloaders, criterion, optimizer, n_epoch, checkpoint_path,\n",
        "          use_image=True, use_text=True):\n",
        "    \"\"\"\n",
        "    Train loop. Performs both training and validation for each epoch.\n",
        "    Saves the best model according to validation loss.\n",
        "\n",
        "    :param model: Pytorch model\n",
        "    :param dataloaders: A dictionary of two dataloaders that load\n",
        "        image, text, and target category;\n",
        "        training dataloader is at key 'Train',\n",
        "        validation dataloader is at key 'Validation'\n",
        "    :param criterion: Classification loss function\n",
        "    :param n_epoch: Number of epochs\n",
        "    :param checkpoint_path: Output checkpoint path including filename\n",
        "    :param use_image: Whether to input image to the model in the forward step\n",
        "    :param use_text: Whether to input text description to the model in the forward step\n",
        "    :return: Tuple of the following\n",
        "    * A list of train accuracies\n",
        "    * A list of validation accuracies\n",
        "    \"\"\"\n",
        "    # best validation accuracy over all epochs\n",
        "    best_accuracy = 0.0\n",
        "    # train/validation accuracy for each epoch\n",
        "    accuracy_dict = {'Train': [], 'Validation': []}\n",
        "    # Each epoch consists of train and validation\n",
        "    phases = ['Train', 'Validation']\n",
        "    for epoch in range(n_epoch):\n",
        "        print('-'*10)\n",
        "        print(f'Epoch {epoch + 1}/{n_epoch}:')\n",
        "        for phase in phases:\n",
        "            if phase == 'Train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            running_loss = 0.0\n",
        "            running_correct = 0\n",
        "            running_total = 0\n",
        "            for image, text, target in dataloaders[phase]:\n",
        "                image, text, target = image.to(device), text.to(device), target.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                if use_image and use_text: # image text model\n",
        "                    output = model(image, text)\n",
        "                elif use_image:            # image model\n",
        "                    output = model(image)\n",
        "                else:                      # text model\n",
        "                    output = model(text)\n",
        "                loss = criterion(output, target)\n",
        "                if phase == 'Train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                preds = torch.argmax(output, dim=1)\n",
        "                running_loss += loss.item()\n",
        "                running_correct += (preds == target).sum().item()\n",
        "                running_total += target.size(0)\n",
        "            epoch_loss = running_loss / len(dataloaders[phase])\n",
        "            epoch_accuracy = running_correct / running_total\n",
        "            accuracy_dict[phase].append(epoch_accuracy)\n",
        "            print(f'{phase} loss:', round(epoch_loss, 4),\n",
        "                  f'\\t{phase} accuracy:', round(epoch_accuracy, 4))\n",
        "            if phase == 'Validation' and epoch_accuracy > best_accuracy:\n",
        "                print('Validation accuracy increases from', round(best_accuracy, 4),\n",
        "                      'to', round(epoch_accuracy, 4))\n",
        "                best_accuracy = epoch_accuracy\n",
        "                # save best model according to validation loss\n",
        "                torch.save(\n",
        "                    {'epoch': epoch,\n",
        "                     'model_state_dict': model.state_dict(),\n",
        "                     'optimizer_state_dict': optimizer.state_dict(),\n",
        "                     'loss': epoch_loss},\n",
        "                    checkpoint_path\n",
        "                )\n",
        "    return accuracy_dict['Train'], accuracy_dict['Validation']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz55Zqi69ff8"
      },
      "source": [
        "## Inference Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "x4b82j2z9ff9"
      },
      "outputs": [],
      "source": [
        "def predict(model, dataloader, use_image=True, use_text=True, val=True):\n",
        "    \"\"\"\n",
        "    Inference loop. Returns the predicted category indices from a single model.\n",
        "    \n",
        "    :param model: Pytorch model.\n",
        "    :param dataloader: A validation or test dataloader.\n",
        "    :param use_image: Whether to input image to the model in the forward step.\n",
        "    :param use_text: Whether to input text description to the model in the forward step.\n",
        "    :param val: Whether to compare predictions with targets and print accuracy.\n",
        "    :return: A numpy array of predicted category indices.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    running_correct = 0\n",
        "    running_total = 0\n",
        "    pred_list = []\n",
        "    for batch in dataloader:\n",
        "        image, text = batch[0].to(device), batch[1].to(device)\n",
        "        if use_image and use_text:\n",
        "            output = model(image, text)\n",
        "        elif use_image:\n",
        "            output = model(image)\n",
        "        else:\n",
        "            output = model(text)\n",
        "        preds = torch.argmax(output, dim=1)\n",
        "        preds = preds.data.cpu().numpy()\n",
        "        if val:\n",
        "            target = batch[2].data.cpu().numpy()\n",
        "            running_correct += (preds == target).sum()\n",
        "            running_total += target.shape[0]\n",
        "        pred_list.append(preds)\n",
        "    pred_list = np.concatenate(pred_list)\n",
        "    if val:\n",
        "        accuracy = running_correct / running_total\n",
        "        print('Accuracy:', round(accuracy, 4))\n",
        "    return pred_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hnh4DX4K9ff9"
      },
      "outputs": [],
      "source": [
        "def predict_voting(models, dataloader, use_image=True, use_text=True,\n",
        "                   val=False, weights=None):\n",
        "    \"\"\"\n",
        "    Inference loop. Perform voting with a number of base models.\n",
        "    Weights are optional. If weights are given, then the predictions are \n",
        "    the categories with the maximum weighted sum of logits from base models.\n",
        "    Otherwise, the predictions are the mode of the categories predicted by base models.\n",
        "    \n",
        "    :param models: A list of Pytorch models.\n",
        "    :param dataloader: A validation or test dataloader.\n",
        "    :param use_image: Whether to input image to the model in the forward step.\n",
        "    :param use_text: Whether to input text description to the model in the forward step.\n",
        "    :param val: Whether to compare predictions with targets and print accuracy.\n",
        "    :param weights: Optional list of weights applied to each model when voting.\n",
        "    :return: A numpy array of predicted category indices.\n",
        "    \"\"\"\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "    running_correct = 0\n",
        "    running_total = 0\n",
        "    pred_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            image, text = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "            preds = []\n",
        "            if weights is None:\n",
        "                for i, model in enumerate(models):\n",
        "                    if use_image and use_text:\n",
        "                        output = model(image, text)\n",
        "                    elif use_image:\n",
        "                        output = model(image)\n",
        "                    else:\n",
        "                        output = model(text)\n",
        "                    pred = torch.argmax(output, dim=1)\n",
        "                    preds.append(pred.data.cpu())\n",
        "                preds = torch.stack(preds)\n",
        "                preds = mode(preds, axis=0, keepdims=False)[0]\n",
        "            else:\n",
        "                for i, model in enumerate(models):\n",
        "                    if use_image and use_text:\n",
        "                        output = model(image, text)\n",
        "                    elif use_image:\n",
        "                        output = model(image)\n",
        "                    else:\n",
        "                        output = model(text)\n",
        "                    output = output * weights[i]\n",
        "                    preds.append(output.data.cpu().numpy())\n",
        "                preds = np.stack(preds)\n",
        "                preds = np.argmax(np.sum(preds, axis=0), axis=1)\n",
        "            if val:\n",
        "                target = batch[2].data.cpu().numpy()\n",
        "                running_correct += (preds == target).sum()\n",
        "                running_total += target.shape[0]\n",
        "            pred_list.append(preds)\n",
        "\n",
        "        pred_list = np.concatenate(pred_list)\n",
        "        if val:\n",
        "            accuracy = running_correct / running_total\n",
        "            print('Accuracy:', round(accuracy, 4))\n",
        "    return pred_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "MpaPOz8m9ff9"
      },
      "outputs": [],
      "source": [
        "### run this locally, for older version of torchtext\n",
        "def index_to_category(pred_list, category_vocab):\n",
        "    \"\"\"\n",
        "    Converts category indices to category names.\n",
        "\n",
        "    :param pred_list: A list or numpy array of predicted category indices.\n",
        "    :param category_vocab: A torchtext Vocab object containing all tokens.\n",
        "    :return: A list of category names.\n",
        "    \"\"\"\n",
        "    categories = category_vocab.itos\n",
        "    pred_list = [categories[int(idx)] for idx in pred_list]\n",
        "    return pred_list\n",
        "\n",
        "\n",
        "### run this on Colab, for new version of torchtext\n",
        "# def index_to_category(pred_list, category_vocab):\n",
        "#     categories = category_vocab.get_itos()\n",
        "#     pred_list = [categories[int(idx)] for idx in pred_list]\n",
        "#     return pred_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlioNbppVjtp"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Jp2lLpL7Vjtp"
      },
      "outputs": [],
      "source": [
        "# configure dataset\n",
        "train_dataframe_path = './dataset/train_cleaned.csv'\n",
        "val_dataframe_path = './dataset/val_cleaned.csv'\n",
        "image_root = './dataset/noisy-images/noisy-images'\n",
        "max_seq = 16\n",
        "batch_size = 48\n",
        "train_noise_prob = 0.5\n",
        "val_noise_prob = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Q1zNY8f8Vjtp"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([transforms.Resize(90),\n",
        "                                      transforms.RandomResizedCrop(80), \n",
        "                                      transforms.RandomHorizontalFlip(), \n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.8193, 0.8041, 0.7969), (0.2224, 0.2341, 0.2369))])\n",
        "train_dataset = ImageTextDataset(train_dataframe_path, image_root,\n",
        "                           text_vocab=description_vocab,\n",
        "                           category_vocab=category_vocab,\n",
        "                           max_seq=max_seq, noise_prob=train_noise_prob,\n",
        "                           transform=train_transform, train=True)\n",
        "val_dataset = ImageTextDataset(val_dataframe_path, image_root,\n",
        "                           text_vocab=description_vocab,\n",
        "                           category_vocab=category_vocab,\n",
        "                           max_seq=max_seq, noise_prob=val_noise_prob,\n",
        "                           transform=train_transform, train=True)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False, drop_last=False\n",
        ")\n",
        "\n",
        "train_dataloaders = {'Train': train_dataloader, 'Validation': val_dataloader}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ZD1rPPhyVjtp"
      },
      "outputs": [],
      "source": [
        "test_dataframe_path = './dataset/test_cleaned.csv'\n",
        "image_root = './dataset/noisy-images/noisy-images'\n",
        "max_seq = 16\n",
        "batch_size = 48\n",
        "test_noise_prob = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "66IohvviVjtp"
      },
      "outputs": [],
      "source": [
        "test_transform = transforms.Compose([transforms.Resize(90),\n",
        "                                     transforms.CenterCrop(80),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.8193, 0.8041, 0.7969), (0.2224, 0.2341, 0.2369))])\n",
        "test_dataset = ImageTextDataset(test_dataframe_path, image_root,\n",
        "                                text_vocab=description_vocab,\n",
        "                                category_vocab=category_vocab,\n",
        "                                max_seq=max_seq, noise_prob=test_noise_prob,\n",
        "                                transform=test_transform, train=False)\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, drop_last=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjYOFfmTVjtp",
        "outputId": "28397b3c-863b-4f9d-eccb-45196012467e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0.8125, 0.7949, 0.7772,  ..., 0.7067, 0.7067, 0.7067],\n",
            "         [0.7772, 0.7772, 0.7596,  ..., 0.7420, 0.7420, 0.7420],\n",
            "         [0.7243, 0.7420, 0.7420,  ..., 0.7420, 0.7420, 0.7420],\n",
            "         ...,\n",
            "         [0.7067, 0.7067, 0.6891,  ..., 0.6891, 0.6891, 0.6714],\n",
            "         [0.6714, 0.6714, 0.6891,  ..., 0.7067, 0.7067, 0.7067],\n",
            "         [0.6538, 0.6714, 0.6891,  ..., 0.7067, 0.7067, 0.6891]],\n",
            "\n",
            "        [[0.8201, 0.8201, 0.7866,  ..., 0.7196, 0.7196, 0.7196],\n",
            "         [0.8033, 0.8033, 0.7866,  ..., 0.7531, 0.7531, 0.7531],\n",
            "         [0.7866, 0.7866, 0.7698,  ..., 0.7531, 0.7531, 0.7531],\n",
            "         ...,\n",
            "         [0.7698, 0.7531, 0.7196,  ..., 0.7531, 0.7531, 0.7363],\n",
            "         [0.7363, 0.7196, 0.7028,  ..., 0.7698, 0.7698, 0.7698],\n",
            "         [0.7196, 0.7196, 0.7028,  ..., 0.7698, 0.7698, 0.7531]],\n",
            "\n",
            "        [[0.8573, 0.8573, 0.8408,  ..., 0.8077, 0.8242, 0.8242],\n",
            "         [0.8408, 0.8242, 0.8077,  ..., 0.8408, 0.8573, 0.8573],\n",
            "         [0.7911, 0.7746, 0.7580,  ..., 0.8408, 0.8573, 0.8573],\n",
            "         ...,\n",
            "         [0.7746, 0.7580, 0.7414,  ..., 0.7580, 0.7580, 0.7414],\n",
            "         [0.7414, 0.7414, 0.7249,  ..., 0.7746, 0.7746, 0.7746],\n",
            "         [0.7249, 0.7249, 0.7249,  ..., 0.7746, 0.7746, 0.7580]]])\n",
            "Image shape: torch.Size([48, 3, 80, 80])\n",
            "tensor([ 15,   8,  13,   7,  28,  15, 246,   1,   1,   1,   1,   1,   1,   1,\n",
            "          1,   1])\n",
            "Text shape: torch.Size([48, 16])\n",
            "Target: tensor(4)\n"
          ]
        }
      ],
      "source": [
        "# sample train data\n",
        "image, text, target = next(iter(train_dataloaders['Train']))\n",
        "print(image[0])\n",
        "print('Image shape:', image.shape)\n",
        "print(text[0])\n",
        "print('Text shape:', text.shape)\n",
        "print('Target:', target[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "jOf4bCWTVjtp"
      },
      "outputs": [],
      "source": [
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U7ravNa9ff-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcXrJzoF9ff-"
      },
      "source": [
        "### Training Resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hjLRiunVjtq"
      },
      "outputs": [],
      "source": [
        "resnet34 = timm.create_model('resnet34', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "resnet34 = resnet34.to(device)\n",
        "optimizer = torch.optim.SGD(resnet34.parameters(), lr=3e-4, momentum=0.9)\n",
        "\n",
        "# checkpoint = torch.load('./models/resnet34.pth')\n",
        "# resnet34.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51O6JwsXVjtq"
      },
      "outputs": [],
      "source": [
        "# train resnet34, takes an hour\n",
        "n_epoch = 60  # can modify\n",
        "checkpoint_path = './models/resnet34.pth'\n",
        "resnet34_train_accuracy, resnet34_val_accuracy = train(\n",
        "    resnet34, train_dataloaders, criterion, optimizer, n_epoch=n_epoch,\n",
        "    checkpoint_path=checkpoint_path, use_image=True, use_text=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKgyzfKZ9ff_"
      },
      "source": [
        "### Training Efficientnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp5oujEQ9ff_"
      },
      "outputs": [],
      "source": [
        "efficientnet = timm.create_model('efficientnetv2_rw_t', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "efficientnet = efficientnet.to(device)\n",
        "optimizer = torch.optim.SGD(efficientnet.parameters(), lr=3e-4, momentum=0.9)\n",
        "\n",
        "# checkpoint = torch.load('./models/efficientnetv2.pth')\n",
        "# efficientnet.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmhDXbUC9ff_",
        "outputId": "554fc297-f052-43cf-8645-660a92a5377b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "Epoch 1/5:\n",
            "Train loss: 0.884 \tTrain accuracy: 0.7888\n",
            "Validation loss: 1.0284 \tValidation accuracy: 0.773\n",
            "Validation accuracy increases from 0.0 to 0.773\n",
            "----------\n",
            "Epoch 2/5:\n",
            "Train loss: 0.8664 \tTrain accuracy: 0.7896\n",
            "Validation loss: 1.0157 \tValidation accuracy: 0.7751\n",
            "Validation accuracy increases from 0.773 to 0.7751\n",
            "----------\n",
            "Epoch 3/5:\n",
            "Train loss: 0.8702 \tTrain accuracy: 0.789\n",
            "Validation loss: 1.0164 \tValidation accuracy: 0.77\n",
            "----------\n",
            "Epoch 4/5:\n",
            "Train loss: 0.8443 \tTrain accuracy: 0.7949\n",
            "Validation loss: 1.0508 \tValidation accuracy: 0.7624\n",
            "----------\n",
            "Epoch 5/5:\n",
            "Train loss: 0.8475 \tTrain accuracy: 0.7937\n",
            "Validation loss: 1.0412 \tValidation accuracy: 0.7735\n"
          ]
        }
      ],
      "source": [
        "# train efficientnet v2, takes an hour\n",
        "n_epoch = 60  # can modify\n",
        "checkpoint_path = './models/efficientnetv2.pth'\n",
        "efficientnet_train_accuracy, efficientnet_val_accuracy = train(\n",
        "    efficientnet, train_dataloaders, criterion, optimizer, n_epoch=n_epoch,\n",
        "    checkpoint_path=checkpoint_path, use_image=True, use_text=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L5zA3u69ff_"
      },
      "source": [
        "### Training Convnext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiYQkRGD9ff_"
      },
      "outputs": [],
      "source": [
        "convnext = timm.create_model('convnext_small_in22k', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "convnext = convnext.to(device)\n",
        "optimizer = torch.optim.SGD(convnext.parameters(), lr=3e-4, momentum=0.9)\n",
        "\n",
        "# checkpoint = torch.load('./models/convnext_small.pth')\n",
        "# convnext.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W431UNPk9ff_",
        "outputId": "7aae56b4-ede1-4d5f-f522-eafe3590ed70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "Epoch 1/20:\n",
            "Train loss: 0.8734 \tTrain accuracy: 0.8035\n",
            "Validation loss: 0.9125 \tValidation accuracy: 0.7899\n",
            "Validation accuracy increases from 0.0 to 0.7899\n",
            "----------\n",
            "Epoch 2/20:\n",
            "Train loss: 0.8743 \tTrain accuracy: 0.803\n",
            "Validation loss: 0.8938 \tValidation accuracy: 0.7933\n",
            "Validation accuracy increases from 0.7899 to 0.7933\n",
            "----------\n",
            "Epoch 3/20:\n",
            "Train loss: 0.8553 \tTrain accuracy: 0.8079\n",
            "Validation loss: 0.8959 \tValidation accuracy: 0.7966\n",
            "Validation accuracy increases from 0.7933 to 0.7966\n",
            "----------\n",
            "Epoch 4/20:\n",
            "Train loss: 0.8607 \tTrain accuracy: 0.8051\n",
            "Validation loss: 0.8934 \tValidation accuracy: 0.797\n",
            "Validation accuracy increases from 0.7966 to 0.797\n",
            "----------\n",
            "Epoch 5/20:\n",
            "Train loss: 0.8619 \tTrain accuracy: 0.8073\n",
            "Validation loss: 0.903 \tValidation accuracy: 0.7936\n",
            "----------\n",
            "Epoch 6/20:\n",
            "Train loss: 0.8579 \tTrain accuracy: 0.8069\n",
            "Validation loss: 0.8975 \tValidation accuracy: 0.7968\n",
            "----------\n",
            "Epoch 7/20:\n",
            "Train loss: 0.856 \tTrain accuracy: 0.807\n",
            "Validation loss: 0.9002 \tValidation accuracy: 0.7936\n",
            "----------\n",
            "Epoch 8/20:\n",
            "Train loss: 0.8564 \tTrain accuracy: 0.8047\n",
            "Validation loss: 0.9053 \tValidation accuracy: 0.7931\n",
            "----------\n",
            "Epoch 9/20:\n",
            "Train loss: 0.8603 \tTrain accuracy: 0.8036\n",
            "Validation loss: 0.8801 \tValidation accuracy: 0.7984\n",
            "Validation accuracy increases from 0.797 to 0.7984\n",
            "----------\n",
            "Epoch 10/20:\n",
            "Train loss: 0.8416 \tTrain accuracy: 0.8096\n",
            "Validation loss: 0.9047 \tValidation accuracy: 0.797\n",
            "----------\n",
            "Epoch 11/20:\n",
            "Train loss: 0.8501 \tTrain accuracy: 0.809\n",
            "Validation loss: 0.9035 \tValidation accuracy: 0.795\n",
            "----------\n",
            "Epoch 12/20:\n",
            "Train loss: 0.8447 \tTrain accuracy: 0.808\n",
            "Validation loss: 0.9024 \tValidation accuracy: 0.794\n",
            "----------\n",
            "Epoch 13/20:\n",
            "Train loss: 0.8527 \tTrain accuracy: 0.8068\n",
            "Validation loss: 0.9048 \tValidation accuracy: 0.7954\n",
            "----------\n",
            "Epoch 14/20:\n",
            "Train loss: 0.8478 \tTrain accuracy: 0.8095\n",
            "Validation loss: 0.8877 \tValidation accuracy: 0.8003\n",
            "Validation accuracy increases from 0.7984 to 0.8003\n",
            "----------\n",
            "Epoch 15/20:\n",
            "Train loss: 0.8385 \tTrain accuracy: 0.815\n",
            "Validation loss: 0.8826 \tValidation accuracy: 0.8047\n",
            "Validation accuracy increases from 0.8003 to 0.8047\n",
            "----------\n",
            "Epoch 16/20:\n",
            "Train loss: 0.8416 \tTrain accuracy: 0.8084\n",
            "Validation loss: 0.907 \tValidation accuracy: 0.7945\n",
            "----------\n",
            "Epoch 17/20:\n",
            "Train loss: 0.8401 \tTrain accuracy: 0.809\n",
            "Validation loss: 0.8944 \tValidation accuracy: 0.8005\n",
            "----------\n",
            "Epoch 18/20:\n",
            "Train loss: 0.8298 \tTrain accuracy: 0.8133\n",
            "Validation loss: 0.8854 \tValidation accuracy: 0.7954\n",
            "----------\n",
            "Epoch 19/20:\n",
            "Train loss: 0.8298 \tTrain accuracy: 0.8131\n",
            "Validation loss: 0.8836 \tValidation accuracy: 0.8005\n",
            "----------\n",
            "Epoch 20/20:\n",
            "Train loss: 0.8261 \tTrain accuracy: 0.8128\n",
            "Validation loss: 0.8901 \tValidation accuracy: 0.8031\n"
          ]
        }
      ],
      "source": [
        "# train convnext small, takes over an hour\n",
        "n_epoch = 60  # can modify\n",
        "checkpoint_path = './models/convnext_small.pth'\n",
        "convnext_train_accuracy, convnext_val_accuracy = train(\n",
        "    convnext, train_dataloaders, criterion, optimizer, n_epoch=n_epoch,\n",
        "    checkpoint_path=checkpoint_path, use_image=True, use_text=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH_pc1XF9ff_"
      },
      "source": [
        "### Training Text Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### run this locally, for older version of torchtext\n",
        "input_size = len(description_vocab.itos)\n",
        "\n",
        "### run this on Colab, for new version of torchtext\n",
        "# input_size = len(description_vocab.get_itos())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfVOdzX59ff_"
      },
      "outputs": [],
      "source": [
        "d_model = 512\n",
        "output_size = 27\n",
        "max_length = 16\n",
        "nhead = 8\n",
        "dim_feedforward = 512\n",
        "num_layers = 6\n",
        "dropout = 0.4\n",
        "\n",
        "transformer = Transformer(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    output_size=output_size,\n",
        "    max_length=max_length,\n",
        "    nhead=nhead,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout)\n",
        "transformer.to(device)\n",
        "optimizer = torch.optim.SGD(transformer.parameters(), lr=3e-4, momentum=0.9)\n",
        "\n",
        "# checkpoint = torch.load('./models/transformer.pth')\n",
        "# transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU4rW7xp9fgA",
        "outputId": "3ea9f753-2fd3-4fdc-d36c-c61ce0a6528c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "Epoch 1/20:\n",
            "Train loss: 0.6854 \tTrain accuracy: 0.8096\n",
            "Validation loss: 0.7625 \tValidation accuracy: 0.806\n",
            "Validation accuracy increases from 0.0 to 0.806\n",
            "----------\n",
            "Epoch 2/20:\n",
            "Train loss: 0.6842 \tTrain accuracy: 0.8132\n",
            "Validation loss: 0.7611 \tValidation accuracy: 0.806\n",
            "----------\n",
            "Epoch 3/20:\n",
            "Train loss: 0.6764 \tTrain accuracy: 0.8131\n",
            "Validation loss: 0.7521 \tValidation accuracy: 0.8072\n",
            "Validation accuracy increases from 0.806 to 0.8072\n",
            "----------\n",
            "Epoch 4/20:\n",
            "Train loss: 0.6762 \tTrain accuracy: 0.8139\n",
            "Validation loss: 0.748 \tValidation accuracy: 0.806\n",
            "----------\n",
            "Epoch 5/20:\n",
            "Train loss: 0.6758 \tTrain accuracy: 0.8137\n",
            "Validation loss: 0.7513 \tValidation accuracy: 0.8044\n",
            "----------\n",
            "Epoch 6/20:\n",
            "Train loss: 0.6792 \tTrain accuracy: 0.8124\n",
            "Validation loss: 0.7467 \tValidation accuracy: 0.8053\n",
            "----------\n",
            "Epoch 7/20:\n",
            "Train loss: 0.6713 \tTrain accuracy: 0.8122\n",
            "Validation loss: 0.7454 \tValidation accuracy: 0.8104\n",
            "Validation accuracy increases from 0.8072 to 0.8104\n",
            "----------\n",
            "Epoch 8/20:\n",
            "Train loss: 0.6715 \tTrain accuracy: 0.813\n",
            "Validation loss: 0.7495 \tValidation accuracy: 0.8088\n",
            "----------\n",
            "Epoch 9/20:\n",
            "Train loss: 0.6727 \tTrain accuracy: 0.8158\n",
            "Validation loss: 0.7421 \tValidation accuracy: 0.809\n",
            "----------\n",
            "Epoch 10/20:\n",
            "Train loss: 0.6698 \tTrain accuracy: 0.8153\n",
            "Validation loss: 0.7424 \tValidation accuracy: 0.812\n",
            "Validation accuracy increases from 0.8104 to 0.812\n",
            "----------\n",
            "Epoch 11/20:\n",
            "Train loss: 0.6651 \tTrain accuracy: 0.8147\n",
            "Validation loss: 0.7415 \tValidation accuracy: 0.8095\n",
            "----------\n",
            "Epoch 12/20:\n",
            "Train loss: 0.6693 \tTrain accuracy: 0.8165\n",
            "Validation loss: 0.7456 \tValidation accuracy: 0.8106\n",
            "----------\n",
            "Epoch 13/20:\n",
            "Train loss: 0.6684 \tTrain accuracy: 0.8171\n",
            "Validation loss: 0.7446 \tValidation accuracy: 0.8097\n",
            "----------\n",
            "Epoch 14/20:\n",
            "Train loss: 0.6611 \tTrain accuracy: 0.8152\n",
            "Validation loss: 0.7452 \tValidation accuracy: 0.8097\n",
            "----------\n",
            "Epoch 15/20:\n",
            "Train loss: 0.6673 \tTrain accuracy: 0.815\n",
            "Validation loss: 0.7434 \tValidation accuracy: 0.8093\n",
            "----------\n",
            "Epoch 16/20:\n",
            "Train loss: 0.6633 \tTrain accuracy: 0.8175\n",
            "Validation loss: 0.7421 \tValidation accuracy: 0.8083\n",
            "----------\n",
            "Epoch 17/20:\n",
            "Train loss: 0.6624 \tTrain accuracy: 0.8168\n",
            "Validation loss: 0.7437 \tValidation accuracy: 0.8072\n",
            "----------\n",
            "Epoch 18/20:\n",
            "Train loss: 0.6595 \tTrain accuracy: 0.8186\n",
            "Validation loss: 0.7433 \tValidation accuracy: 0.8095\n",
            "----------\n",
            "Epoch 19/20:\n",
            "Train loss: 0.6632 \tTrain accuracy: 0.8178\n",
            "Validation loss: 0.7405 \tValidation accuracy: 0.8123\n",
            "Validation accuracy increases from 0.812 to 0.8123\n",
            "----------\n",
            "Epoch 20/20:\n",
            "Train loss: 0.6572 \tTrain accuracy: 0.8171\n",
            "Validation loss: 0.7433 \tValidation accuracy: 0.8072\n"
          ]
        }
      ],
      "source": [
        "# train transformer, takes an hour\n",
        "n_epoch = 80  # can modify\n",
        "checkpoint_path = './models/transformer.pth'\n",
        "transformer_train_accuracy, transformer_val_accuracy = train(\n",
        "    transformer, train_dataloaders, criterion, optimizer, n_epoch=n_epoch,\n",
        "    checkpoint_path=checkpoint_path, use_image=False, use_text=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKvgyecc9fgA"
      },
      "source": [
        "### Training Resnet + Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE6li6qe9fgA",
        "outputId": "ec8d852c-9e67-46e9-b5a8-72f22f78d3b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create resnet34 and load checkpoint, no grad\n",
        "resnet34 = timm.create_model('resnet34', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "resnet34 = resnet34.to(device)\n",
        "checkpoint = torch.load('./models/resnet34.pth')\n",
        "resnet34.load_state_dict(checkpoint['model_state_dict'])\n",
        "for param in resnet34.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# create transformer and load checkpoint, no grad\n",
        "# input_size defined before\n",
        "d_model = 512\n",
        "output_size = 27\n",
        "max_length = 16\n",
        "nhead = 8\n",
        "dim_feedforward = 512\n",
        "num_layers = 6\n",
        "dropout = 0.4\n",
        "\n",
        "transformer = Transformer(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    output_size=output_size,\n",
        "    max_length=max_length,\n",
        "    nhead=nhead,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout)\n",
        "transformer.to(device)\n",
        "checkpoint = torch.load('./models/transformer.pth')\n",
        "transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "for param in transformer.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# create resnet34 + transformer model and load checkpoint\n",
        "resnet34_transformer = ImageTextClassifier(resnet34, transformer, hidden_size=256, num_classes=27, dropout=0.3)\n",
        "resnet34_transformer.to(device)\n",
        "optimizer = torch.optim.SGD(resnet34_transformer.parameters(), lr=3e-4, momentum=0.9)\n",
        "\n",
        "# checkpoint = torch.load('./models/resnet34_transformer.pth')\n",
        "# resnet34_transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lFom8CB9fgA",
        "outputId": "4c785852-b77a-4e79-9dc1-1b5cde8f8de1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "Epoch 1/10:\n",
            "Train loss: 0.3185 \tTrain accuracy: 0.9089\n",
            "Validation loss: 0.4714 \tValidation accuracy: 0.8964\n",
            "Validation accuracy increases from 0.0 to 0.8964\n",
            "----------\n",
            "Epoch 2/10:\n",
            "Train loss: 0.314 \tTrain accuracy: 0.9124\n",
            "Validation loss: 0.4625 \tValidation accuracy: 0.896\n",
            "----------\n",
            "Epoch 3/10:\n",
            "Train loss: 0.3077 \tTrain accuracy: 0.9144\n",
            "Validation loss: 0.4693 \tValidation accuracy: 0.8962\n",
            "----------\n",
            "Epoch 4/10:\n",
            "Train loss: 0.3106 \tTrain accuracy: 0.9119\n",
            "Validation loss: 0.4722 \tValidation accuracy: 0.8964\n",
            "----------\n",
            "Epoch 5/10:\n",
            "Train loss: 0.3077 \tTrain accuracy: 0.9134\n",
            "Validation loss: 0.4652 \tValidation accuracy: 0.8964\n",
            "----------\n",
            "Epoch 6/10:\n",
            "Train loss: 0.3183 \tTrain accuracy: 0.913\n",
            "Validation loss: 0.4651 \tValidation accuracy: 0.8944\n",
            "----------\n",
            "Epoch 7/10:\n",
            "Train loss: 0.316 \tTrain accuracy: 0.9123\n",
            "Validation loss: 0.4568 \tValidation accuracy: 0.8946\n",
            "----------\n",
            "Epoch 8/10:\n",
            "Train loss: 0.3137 \tTrain accuracy: 0.9118\n",
            "Validation loss: 0.4695 \tValidation accuracy: 0.8941\n",
            "----------\n",
            "Epoch 9/10:\n",
            "Train loss: 0.3129 \tTrain accuracy: 0.9132\n",
            "Validation loss: 0.4682 \tValidation accuracy: 0.8976\n",
            "Validation accuracy increases from 0.8964 to 0.8976\n",
            "----------\n",
            "Epoch 10/10:\n",
            "Train loss: 0.3067 \tTrain accuracy: 0.915\n",
            "Validation loss: 0.4437 \tValidation accuracy: 0.8983\n",
            "Validation accuracy increases from 0.8976 to 0.8983\n"
          ]
        }
      ],
      "source": [
        "# train, takes 30 min\n",
        "n_epoch = 60  # can modify\n",
        "checkpoint_path = './models/resnet34_transformer.pth'\n",
        "resnet34_transformer_train_accuracy, resnet34_transformer_val_accuracy = train(\n",
        "    resnet34_transformer, train_dataloaders, criterion, optimizer, n_epoch=n_epoch,\n",
        "    checkpoint_path=checkpoint_path, use_image=True, use_text=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHrq7cu49fgA"
      },
      "source": [
        "### Training Efficientnet + Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fSnXc7q9fgA"
      },
      "outputs": [],
      "source": [
        "# create efficientnet v2 and load checkpoint, no grad\n",
        "efficientnet = timm.create_model('efficientnetv2_rw_t', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "efficientnet = efficientnet.to(device)\n",
        "checkpoint = torch.load('./models/efficientnetv2.pth')\n",
        "efficientnet.load_state_dict(checkpoint['model_state_dict'])\n",
        "for param in efficientnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# create transformer and load checkpoint, no grad\n",
        "# input_size defined before\n",
        "d_model = 512\n",
        "output_size = 27\n",
        "max_length = 16\n",
        "nhead = 8\n",
        "dim_feedforward = 512\n",
        "num_layers = 6\n",
        "dropout = 0.4\n",
        "\n",
        "transformer = Transformer(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    output_size=output_size,\n",
        "    max_length=max_length,\n",
        "    nhead=nhead,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout)\n",
        "transformer.to(device)\n",
        "checkpoint = torch.load('./models/transformer.pth')\n",
        "transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "for param in transformer.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# create efficientnet v2 + transformer and load checkpoint\n",
        "efficientnet_transformer = ImageTextClassifier(efficientnet, transformer, hidden_size=256, num_classes=27, dropout=0.3)\n",
        "efficientnet_transformer.to(device)\n",
        "optimizer = torch.optim.SGD(efficientnet_transformer.parameters(), lr=3e-4, momentum=0.9)\n",
        "\n",
        "# checkpoint = torch.load('./models/efficientnetv2_transformer.pth')\n",
        "# efficientnet_transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZnYVADx9fgA",
        "outputId": "70ede617-a581-4dc2-aac5-82a0c8c3cd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "Epoch 1/10:\n",
            "Train loss: 0.313 \tTrain accuracy: 0.9122\n",
            "Validation loss: 0.4317 \tValidation accuracy: 0.8927\n",
            "Validation accuracy increases from 0.0 to 0.8927\n",
            "----------\n",
            "Epoch 2/10:\n",
            "Train loss: 0.317 \tTrain accuracy: 0.9102\n",
            "Validation loss: 0.416 \tValidation accuracy: 0.8962\n",
            "Validation accuracy increases from 0.8927 to 0.8962\n",
            "----------\n",
            "Epoch 3/10:\n",
            "Train loss: 0.3163 \tTrain accuracy: 0.9132\n",
            "Validation loss: 0.426 \tValidation accuracy: 0.8962\n",
            "----------\n",
            "Epoch 4/10:\n",
            "Train loss: 0.3144 \tTrain accuracy: 0.9098\n",
            "Validation loss: 0.4408 \tValidation accuracy: 0.8877\n",
            "----------\n",
            "Epoch 5/10:\n",
            "Train loss: 0.3142 \tTrain accuracy: 0.9122\n",
            "Validation loss: 0.4157 \tValidation accuracy: 0.8992\n",
            "Validation accuracy increases from 0.8962 to 0.8992\n",
            "----------\n",
            "Epoch 6/10:\n",
            "Train loss: 0.319 \tTrain accuracy: 0.9085\n",
            "Validation loss: 0.4188 \tValidation accuracy: 0.8944\n",
            "----------\n",
            "Epoch 7/10:\n",
            "Train loss: 0.3199 \tTrain accuracy: 0.9099\n",
            "Validation loss: 0.4278 \tValidation accuracy: 0.8918\n",
            "----------\n",
            "Epoch 8/10:\n",
            "Train loss: 0.3195 \tTrain accuracy: 0.9124\n",
            "Validation loss: 0.4374 \tValidation accuracy: 0.8904\n",
            "----------\n",
            "Epoch 9/10:\n",
            "Train loss: 0.3116 \tTrain accuracy: 0.9129\n",
            "Validation loss: 0.4266 \tValidation accuracy: 0.893\n",
            "----------\n",
            "Epoch 10/10:\n",
            "Train loss: 0.3125 \tTrain accuracy: 0.9107\n",
            "Validation loss: 0.4122 \tValidation accuracy: 0.8964\n"
          ]
        }
      ],
      "source": [
        "# train, takes 30 min\n",
        "n_epoch = 60  # can modify\n",
        "checkpoint_path = './models/efficientnetv2_transformer.pth'\n",
        "efficientnet_transformer_train_accuracy, efficientnet_transformer_val_accuracy = train(\n",
        "    efficientnet_transformer, train_dataloaders, criterion, optimizer, n_epoch=n_epoch,\n",
        "    checkpoint_path=checkpoint_path, use_image=True, use_text=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtZoNyTT9fgB"
      },
      "source": [
        "### Training Convnext + Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwzJhiiS9fgB",
        "outputId": "93f5923c-4c3e-41b6-81a2-6422a06f2b31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create convnext small and load checkpoint, no grad\n",
        "convnext = timm.create_model('convnext_small_in22k', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "convnext = convnext.to(device)\n",
        "checkpoint = torch.load('./models/convnext_small.pth')\n",
        "convnext.load_state_dict(checkpoint['model_state_dict'])\n",
        "for param in convnext.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# create transformer and load checkpoint, no grad\n",
        "# input_size defined before\n",
        "d_model = 512\n",
        "output_size = 27\n",
        "max_length = 16\n",
        "nhead = 8\n",
        "dim_feedforward = 512\n",
        "num_layers = 6\n",
        "dropout = 0.4\n",
        "\n",
        "transformer = Transformer(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    output_size=output_size,\n",
        "    max_length=max_length,\n",
        "    nhead=nhead,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout)\n",
        "transformer.to(device)\n",
        "checkpoint = torch.load('./models/transformer.pth')\n",
        "transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "for param in transformer.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# create convnext small + transformer model and load checkpoint\n",
        "convnext_transformer = ImageTextClassifier(convnext, transformer, hidden_size=256, num_classes=27, dropout=0.3)\n",
        "convnext_transformer.to(device)\n",
        "optimizer = torch.optim.SGD(convnext_transformer.parameters(), lr=3e-4, momentum=0.9)\n",
        "\n",
        "# checkpoint = torch.load('./models/convnext_small_transformer.pth')\n",
        "# convnext_transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA77bLai9fgB",
        "outputId": "58cfef7f-6bae-441f-bd34-68d93dc6bd1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "Epoch 1/10:\n",
            "Train loss: 0.3127 \tTrain accuracy: 0.9102\n",
            "Validation loss: 0.3913 \tValidation accuracy: 0.9073\n",
            "Validation accuracy increases from 0.0 to 0.9073\n",
            "----------\n",
            "Epoch 2/10:\n",
            "Train loss: 0.3254 \tTrain accuracy: 0.9083\n",
            "Validation loss: 0.3777 \tValidation accuracy: 0.9043\n",
            "----------\n",
            "Epoch 3/10:\n",
            "Train loss: 0.3185 \tTrain accuracy: 0.9082\n",
            "Validation loss: 0.3711 \tValidation accuracy: 0.9108\n",
            "Validation accuracy increases from 0.9073 to 0.9108\n",
            "----------\n",
            "Epoch 4/10:\n",
            "Train loss: 0.3192 \tTrain accuracy: 0.9099\n",
            "Validation loss: 0.3796 \tValidation accuracy: 0.9073\n",
            "----------\n",
            "Epoch 5/10:\n",
            "Train loss: 0.3205 \tTrain accuracy: 0.9102\n",
            "Validation loss: 0.3712 \tValidation accuracy: 0.9098\n",
            "----------\n",
            "Epoch 6/10:\n",
            "Train loss: 0.3254 \tTrain accuracy: 0.9066\n",
            "Validation loss: 0.3697 \tValidation accuracy: 0.9078\n",
            "----------\n",
            "Epoch 7/10:\n",
            "Train loss: 0.3142 \tTrain accuracy: 0.9108\n",
            "Validation loss: 0.3757 \tValidation accuracy: 0.9057\n",
            "----------\n",
            "Epoch 8/10:\n",
            "Train loss: 0.3131 \tTrain accuracy: 0.9106\n",
            "Validation loss: 0.3752 \tValidation accuracy: 0.9061\n",
            "----------\n",
            "Epoch 9/10:\n",
            "Train loss: 0.3189 \tTrain accuracy: 0.9105\n",
            "Validation loss: 0.3698 \tValidation accuracy: 0.9075\n",
            "----------\n",
            "Epoch 10/10:\n",
            "Train loss: 0.317 \tTrain accuracy: 0.9097\n",
            "Validation loss: 0.3736 \tValidation accuracy: 0.9085\n"
          ]
        }
      ],
      "source": [
        "# train, takes 30 min\n",
        "n_epoch = 60  # can modify\n",
        "checkpoint_path = './models/convnext_small_transformer.pth'\n",
        "convnext_transformer_train_accuracy, convnext_transformer_val_accuracy = train(\n",
        "    convnext_transformer, train_dataloaders, criterion, optimizer, n_epoch=n_epoch,\n",
        "    checkpoint_path=checkpoint_path, use_image=True, use_text=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbDOhrFT9fgB"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP-rV_259fgB"
      },
      "source": [
        "Voting ensemble using the three base models: Resnet34 + Transformer, Efficient V2 + Ensemble, and Convnext Small + Ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### run this locally, for older version of torchtext\n",
        "input_size = len(description_vocab.itos)\n",
        "\n",
        "### run this on Colab, for new version of torchtext\n",
        "# input_size = len(description_vocab.get_itos())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k4Wb3mN9fgB",
        "outputId": "ac461793-7196-43da-b6a5-0d8f829f621a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create resnet34 and load checkpoint\n",
        "resnet34 = timm.create_model('resnet34', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "resnet34 = resnet34.to(device)\n",
        "\n",
        "# create transformer and load checkpoint\n",
        "# input_size defined above\n",
        "d_model = 512\n",
        "output_size = 27\n",
        "max_length = 16\n",
        "nhead = 8\n",
        "dim_feedforward = 512\n",
        "num_layers = 6\n",
        "dropout = 0.4\n",
        "\n",
        "transformer = Transformer(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    output_size=output_size,\n",
        "    max_length=max_length,\n",
        "    nhead=nhead,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout)\n",
        "transformer.to(device)\n",
        "\n",
        "# create resnet34 + transformer model and load checkpoint\n",
        "resnet34_transformer = ImageTextClassifier(resnet34, transformer, hidden_size=256, num_classes=27, dropout=0.3)\n",
        "resnet34_transformer.to(device)\n",
        "checkpoint = torch.load('./models/resnet34_transformer.pth')\n",
        "resnet34_transformer.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlHWj6yM9fgB",
        "outputId": "d324950e-190f-4fc7-890a-87845eb30aea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create resnet34 and load checkpoint\n",
        "efficientnet = timm.create_model('efficientnetv2_rw_t', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "efficientnet = efficientnet.to(device)\n",
        "\n",
        "# create transformer and load checkpoint\n",
        "# input_size defined above\n",
        "d_model = 512\n",
        "output_size = 27\n",
        "max_length = 16\n",
        "nhead = 8\n",
        "dim_feedforward = 512\n",
        "num_layers = 6\n",
        "dropout = 0.4\n",
        "\n",
        "transformer = Transformer(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    output_size=output_size,\n",
        "    max_length=max_length,\n",
        "    nhead=nhead,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout)\n",
        "transformer.to(device)\n",
        "\n",
        "# create efficientnet v2 + transformer model and load checkpoint\n",
        "efficientnet_transformer = ImageTextClassifier(efficientnet, transformer, hidden_size=256, num_classes=27, dropout=0.3)\n",
        "efficientnet_transformer.to(device)\n",
        "checkpoint = torch.load('./models/efficientnetv2_transformer.pth')\n",
        "efficientnet_transformer.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prdyCql39fgC",
        "outputId": "599626d0-c29c-4b3e-848c-16fb0b5c74d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create resnet34 and load checkpoint\n",
        "convnext = timm.create_model('convnext_small_in22k', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "convnext = convnext.to(device)\n",
        "\n",
        "# create transformer and load checkpoint\n",
        "# input_size defined above\n",
        "d_model = 512\n",
        "output_size = 27\n",
        "max_length = 16\n",
        "nhead = 8\n",
        "dim_feedforward = 512\n",
        "num_layers = 6\n",
        "dropout = 0.4\n",
        "\n",
        "transformer = Transformer(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    output_size=output_size,\n",
        "    max_length=max_length,\n",
        "    nhead=nhead,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout)\n",
        "transformer.to(device)\n",
        "\n",
        "# create convnext small + transformer model and load checkpoint\n",
        "convnext_transformer = ImageTextClassifier(convnext, transformer, hidden_size=256, num_classes=27, dropout=0.3)\n",
        "convnext_transformer.to(device)\n",
        "checkpoint = torch.load('./models/convnext_small_transformer.pth')\n",
        "convnext_transformer.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qz2xFGiY9fgC",
        "outputId": "b1081b3c-fa97-4db1-b266-b81b266103ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation started...\n",
            "Resnet34 + Transformer Accuracy: 0.9036\n",
            "Efficient V2 + Transformer Accuracy: 0.9008\n",
            "Convnext Small + Transformer Accuracy: 0.9115\n",
            "Validation completed\n"
          ]
        }
      ],
      "source": [
        "# base models validation\n",
        "print('Validation started...')\n",
        "print('Resnet34 + Transformer ', end='')\n",
        "pred_list = predict(resnet34_transformer, val_dataloader, use_image=True, use_text=True, val=True)\n",
        "print('Efficient V2 + Transformer ', end='')\n",
        "pred_list = predict(efficientnet_transformer, val_dataloader, use_image=True, use_text=True, val=True)\n",
        "print('Convnext Small + Transformer ', end='')\n",
        "pred_list = predict(convnext_transformer, val_dataloader, use_image=True, use_text=True, val=True)\n",
        "print('Validation completed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPVBwtWP9fgC",
        "outputId": "5c35c0f5-aad6-479f-dcc9-dcac9b773114"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation started...\n",
            "Voting Accuracy: 0.9142\n",
            "Validation completed\n"
          ]
        }
      ],
      "source": [
        "# weighted voting ensemble, use validation accuracies as weights\n",
        "models = [resnet34_transformer, efficientnet_transformer, convnext_transformer]\n",
        "weights = [0.9036, 0.9008, 0.9115]\n",
        "\n",
        "# weighted voting ensemble validation\n",
        "print('Validation started...')\n",
        "print('Voting ', end='')\n",
        "pred_list = predict_voting(models, val_dataloader, use_image=True, use_text=True, val=True, weights=weights)\n",
        "print('Validation completed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja6xwYHz9fgC",
        "outputId": "04f3ad75-75e5-4e7e-f5a6-eb780ea823c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inference started...\n",
            "Inference completed\n",
            "Converting category indices to category names...\n"
          ]
        }
      ],
      "source": [
        "# weighted voting ensemble testing\n",
        "print('Inference started...')\n",
        "pred_list = predict_voting(models, test_dataloader, use_image=True, use_text=True, val=False, weights=weights)\n",
        "print('Inference completed')\n",
        "\n",
        "print('Converting category indices to category names...')\n",
        "pred_list = index_to_category(pred_list, category_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP90jD9G9fgC"
      },
      "outputs": [],
      "source": [
        "# prediction file to be submitted\n",
        "test_data = pd.read_csv('./dataset/test_cleaned.csv')\n",
        "pred_data = test_data[['id']]\n",
        "pred_data.insert(1, 'category', pred_list)\n",
        "pred_data.to_csv('./dataset/predict.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "cs480",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
